# Phase 2 要件定義: Safe Contextual Bandit最適化

## 1. 概要

### 1.1 位置づけ

Phase 2は研究テーマの核心部分:

> **「HAR不確実度を活用したBLE広告のSafe Contextual Bandit最適化」**

Phase 1で確立した評価指標とベースラインを用いて、**オンライン学習による自律的な最適化**を実現する。

### 1.2 新規性

1. HAR不確実度をContextual Banditの「コンテキスト」として初めて使用
2. Pout(τ)を「制約」として扱うSafe Bandit定式化
3. アプリ層（HAR）→通信層（BLE）のクロスレイヤー最適化

### 1.3 前提条件（Phase 1からの引き継ぎ）

| 項目 | Phase 1成果物 |
|------|--------------|
| 評価指標 | Pout(τ), μC/event の測定パイプライン |
| ベースライン | CCS→T_adv写像テーブル |
| 実測データ | μC辞書、Pout(τ)実測値 |

---

## 2. 問題定式化

### 2.1 Safe Contextual Bandit問題

**目的関数**:
```
π* = argmax_π  E_{(U,S)~D} [ r(π(U,S)) ]
     subject to E_{(U,S)~D} [ g(π(U,S)) ] ≤ δ
```

- π: コンテキスト→行動の方策
- r: 報酬（-μC）
- g: 制約関数（Pout(τ)）
- δ: 許容違反率

### 2.2 構成要素

| 要素 | 記号 | 定義 | 詳細 |
|------|------|------|------|
| **コンテキスト** | x = (U, S) | HAR不確実度、安定度 | [0,1] × [0,1] |
| **行動** | a ∈ A | 広告間隔 T_adv | {100, 500, 1000, 2000} ms |
| **報酬** | r(a) | -μC(a) | エネルギー最小化 |
| **制約** | g(a) | Pout(τ \| a) | QoS保証 |
| **方策** | π(x) | コンテキスト→行動 | 学習対象 |

### 2.3 報酬関数

**イベント電荷**:
```
μC(a) = q_adv × (D / T_adv)
```
- q_adv: 1広告あたりの電荷（Phase 1実測から取得）
- D: イベント持続時間
- T_adv: 広告間隔

**報酬**:
```
r(a) = -μC(a)  # エネルギー最小化 = 報酬最大化
```

### 2.4 制約関数

**Pout(τ | a)**:
```
Pout(τ | a) = P(初回受信遅延 > τ | T_adv = a)
            ≈ (1 - p_d)^⌊τ / T_adv⌋
```
- p_d: 1広告の受信成功確率（Phase 1実測から取得）
- τ: 許容遅延（例: 2秒）

**制約**:
```
g(a) = Pout(τ | a) ≤ δ
```
- δ: 許容違反率（例: 5%）

---

## 3. アルゴリズム

### 3.1 候補アルゴリズム

| アルゴリズム | 特徴 | 適用可能性 |
|-------------|------|-----------|
| **Conservative UCB** | 制約を保守的に扱う | ◎ 最有力 |
| **SafeOpt** | 安全領域を段階的に拡張 | ○ 連続行動空間向け |
| **Constrained Thompson Sampling** | ベイズ的制約処理 | ○ サンプル効率良い |
| **Lagrangian Bandit** | ラグランジュ乗数で制約処理 | △ 収束遅い可能性 |

### 3.2 Safe-UCB型アルゴリズム（基本設計）

```python
for each round t:
    # 1. コンテキスト観測
    observe context x_t = (U_t, S_t)

    # 2. 各腕の評価
    for each arm a in A:
        # 報酬のUCB（楽観的）
        UCB_reward[a] = μ_r_hat(a, x_t) + c * sqrt(log(t) / N(a, x_t))

        # 制約のLCB（悲観的 = 安全側）
        LCB_constraint[a] = μ_g_hat(a, x_t) - c * sqrt(log(t) / N(a, x_t))

    # 3. 安全な腕の中から最良を選択
    safe_arms = { a : LCB_constraint[a] ≤ δ }
    if safe_arms is empty:
        a_t = fallback_arm  # 100ms（最も安全）
    else:
        a_t = argmax { UCB_reward[a] : a in safe_arms }

    # 4. 行動実行・観測・更新
    execute a_t
    observe r_t, g_t
    update μ_r_hat, μ_g_hat, N
```

### 3.3 Warm-Start（Phase 1からの引き継ぎ）

```python
# Phase 1の写像テーブルで初期化
initial_policy = {
    (U < 0.30, S > 0.60): 2000ms,  # 高CCS
    (0.30 ≤ U < 0.60, ...): 500ms,  # 中CCS
    (U ≥ 0.60, ...): 100ms,  # 低CCS
}

# 初期推定値をPhase 1実測で設定
μ_r_hat[a] = Phase1_μC[a]  # 報酬の事前推定
μ_g_hat[a] = Phase1_Pout[a]  # 制約の事前推定
```

---

## 4. 評価指標（KPI）

### 4.1 一次KPI（Phase 1から継続）

| KPI | 定義 | Phase 2での評価 |
|-----|------|----------------|
| **Pout(τ)** | P(TL > τ) | 制約δを常に満たすか |
| **μC/event** | イベントあたり電荷 | Phase 1ベースラインより改善 |
| **TL_p95** | 遅延の95パーセンタイル | 参考値 |

### 4.2 Phase 2固有KPI

| KPI | 定義 | 目標 |
|-----|------|------|
| **累積後悔（Regret）** | Σ(r* - r_t) | 亜線形（sublinear）成長 |
| **制約違反率** | #{g_t > δ} / T | ≤ δ（長期的に） |
| **収束速度** | 最適腕選択率が安定するまでのラウンド数 | 参考値 |

---

## 5. 実験計画

### 5.1 オフライン評価（シミュレーション）

| 実験 | 内容 | 目的 |
|------|------|------|
| Regret比較 | Safe-UCB vs UCB vs Fixed | 学習効率の検証 |
| 制約満足度 | 違反率の推移 | 安全性の検証 |
| Warm-Start効果 | With/Without Phase 1初期化 | 引き継ぎの価値 |

### 5.2 オンライン評価（実機）

| 実験 | 内容 | 目的 |
|------|------|------|
| 長時間運用 | 数時間〜1日の連続運用 | 実用性の検証 |
| 環境変化適応 | E1→E2切替時の挙動 | 適応性の検証 |
| 端末差異 | 複数スマホでの評価 | 汎用性の検証 |

---

## 6. 成功基準

### 6.1 最低ライン（論文として成立）

1. Safe-UCBがPout(τ) ≤ δを維持しながら学習
2. 固定方策（Phase 1ベースライン）よりμC改善
3. 累積後悔が亜線形成長

### 6.2 理想ライン（トップカンファレンス級）

1. 上記 + 複数環境での適応性実証
2. 上記 + 理論的後悔バウンドの導出
3. 上記 + 実機での長時間安定運用

---

## 7. リスクと対策

| リスク | 兆候 | 対策 |
|--------|------|------|
| 制約違反頻発 | Pout > δが連続 | Fallback（100ms）へ退避 |
| 学習初期の後悔 | 違反スパイク | Warm-Start強化 |
| 収束しない | 推定値が発散 | 学習率調整、正則化 |
| 環境変化に追従できない | 最適腕が変化 | 減衰係数導入（非定常対応） |

---

## 8. 成果物

| 成果物 | 説明 |
|--------|------|
| Safe-UCBアルゴリズム実装 | ESP32上で動作するC++実装 |
| オフラインシミュレータ | Python実装、ログリプレイ機能 |
| 論文（修論/ジャーナル） | 定式化、アルゴリズム、実験結果 |
| 実験ログ・再現パッケージ | Runbook、データ、スクリプト |

---

## 9. Phase 3への接続

Phase 2の成果物は、Phase 3（Semantic-driven Communication）で以下のように発展:

| Phase 2成果物 | Phase 3での発展 |
|---------------|----------------|
| Safe-UCB | マルチ制約、マルチ目的への拡張 |
| コンテキスト(U, S) | アプリ緊急度、端末状態の追加 |
| 単一デバイス最適化 | マルチデバイス協調 |
| BLE広告間隔 | Tx電力、PHY、チャネル選択への拡張 |

---

## 10. 参考文献（Phase 2で参照）

- Sui, Y., et al. "Safe Exploration for Optimization with Gaussian Processes" (SafeOpt)
- Amani, S., et al. "Linear Stochastic Bandits Under Safety Constraints"
- Kazerouni, A., et al. "Conservative Contextual Linear Bandits"

---

*Last updated: 2025-11-27*
